import streamlit as st
from PIL import Image
import numpy as np
import torch
from torchvision import transforms
import cv2
import os
import traceback
import sys
import itertools

# --- 경로 문제 해결을 위한 코드 ---
# 현재 파일의 절대 경로를 가져옵니다.
current_dir = os.path.dirname(os.path.abspath(__file__))
# Fast-AgingGAN 폴더의 절대 경로를 계산합니다.
gan_module_path = os.path.join(current_dir, 'Fast-AgingGAN')

# 계산된 경로를 sys.path에 추가합니다.
if gan_module_path not in sys.path:
    sys.path.append(gan_module_path)

# --- 이제 gan_module을 import할 수 있습니다. ---
from gan_module import AgingGAN
from models import Discriminator, Generator

# ---------------- 하이퍼파라미터 직접 정의 ----------------
hparams = {
    'ngf': 32,
    'ndf': 32,
    'n_blocks': 9,
    'n_layers_D': 3,
    'img_size': 128
}

# ---------------- 페이지 설정 ----------------
st.set_page_config(page_title="실시간 미래 얼굴 생성 AI", layout="centered")
st.title("실시간 미래 얼굴 생성 AI")

# ---------------- 세션 상태 초기화 ----------------
if "page" not in st.session_state:
    st.session_state.page = "home"
if "last_image_path" not in st.session_state:
    st.session_state.last_image_path = None

# ---------------- 장치 설정 ----------------
device = "cuda" if torch.cuda.is_available() else "cpu"

# ---------------- 얼굴 감지기 로드 ----------------
# OpenCV에서 제공하는 Haar Cascade를 사용합니다.
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
if face_cascade.empty():
    st.error("얼굴 감지 모델 파일(haarcascade_frontalface_default.xml)을 로드할 수 없습니다.")
    st.stop()

# ---------------- 모델 로드 ----------------
try:
    aging_model = AgingGAN(hparams).to(device)
    model_path = r"C:\streamlit_file2\file_streamlit\Fast-AgingGAN\pretrained_model\state_dict.pth"

    if os.path.exists(model_path):
        # 가중치 파일 로드
        # weights_only=True로 설정하여 FutureWarning를 해결합니다.
        checkpoint = torch.load(model_path, map_location=device, weights_only=True)

        # PyTorch Lightning 체크포인트 구조에 맞게 state_dict 추출
        if 'state_dict' in checkpoint:
            state_dict = checkpoint['state_dict']
        else:
            state_dict = checkpoint

        # 키 이름 수동 매핑
        new_state_dict = {}
        for k, v in state_dict.items():
            # 'model.'로 시작하는 모든 키에 'genA2B.' 접두사 추가
            new_key = "genA2B." + k
            new_state_dict[new_key] = v

        # 수정된 딕셔너리를 모델에 로드
        # strict=False 옵션으로 누락된 Discriminator 키를 무시합니다.
        aging_model.load_state_dict(new_state_dict, strict=False)

        # 모델을 평가 모드로 설정
        aging_model.eval()
        st.success("GAN 모델 가중치 로드 완료! ✨")
    else:
        st.warning(f"모델 파일이 존재하지 않습니다: {model_path}")

except Exception as e:
    st.error(f"모델 로드 실패: {e}")
    st.code(traceback.format_exc())

# ---------------- 홈 페이지 ----------------
if st.session_state.page == "home":
    st.markdown("""
        <div style='margin-bottom:10px; padding:20px; border-radius:15px; background-color:#f0f9ff; text-align:center;'>
            <p>카메라로 얼굴을 찍고 미래 얼굴을 확인할 수 있습니다.</p>
        </div>
    """, unsafe_allow_html=True)

    if st.button("사진 찍어 미래 얼굴 보기"):
        st.session_state.page = "camera_internal"
        st.rerun()

# ---------------- 카메라 페이지 ----------------
elif st.session_state.page == "camera_internal":
    st.markdown("<h3 style='text-align:center;'>카메라 촬영</h3>", unsafe_allow_html=True)
    uploaded_image = st.camera_input("카메라로 사진을 찍어주세요")

    if uploaded_image is not None:
        try:
            # 바이트 배열로 이미지 로드
            file_bytes = np.asarray(bytearray(uploaded_image.read()), dtype=np.uint8)
            img_bgr = cv2.imdecode(file_bytes, cv2.IMREAD_COLOR)

            if img_bgr is not None:
                # 얼굴 감지
                gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
                # 얼굴 감지 매개변수 조정
                faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))

                cropped_face_pil = None
                if len(faces) > 0:
                    # 가장 큰 얼굴을 선택
                    (x, y, w, h) = sorted(faces, key=lambda f: f[2] * f[3], reverse=True)[0]
                    # 얼굴 부분만 자르기
                    cropped_face_bgr = img_bgr[y:y+h, x:x+w]
                    cropped_face_pil = Image.fromarray(cv2.cvtColor(cropped_face_bgr, cv2.COLOR_BGR2RGB))
                    st.image(cropped_face_pil, caption="감지된 얼굴", width='stretch')
                else:
                    st.warning("얼굴을 감지할 수 없습니다. 전체 이미지를 사용합니다.")
                    cropped_face_pil = Image.fromarray(cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB))
                    st.image(cropped_face_pil, caption="원본 사진 (얼굴 미감지)", width='stretch')
                
                # 얼굴이 감지되었거나 전체 이미지가 준비된 경우에만 생성 버튼 표시
                if cropped_face_pil is not None and st.button("미래 얼굴 생성"):
                    try:
                        transform = transforms.Compose([
                            transforms.Resize((hparams['img_size'], hparams['img_size'])),
                            transforms.ToTensor(),
                            transforms.Normalize([0.5]*3, [0.5]*3)
                        ])
                        input_tensor = transform(cropped_face_pil).unsqueeze(0).to(device)

                        with torch.no_grad():
                            output_tensor = aging_model(input_tensor)

                        # 텐서를 PIL 이미지로 변환
                        output_tensor = (output_tensor.squeeze(0).cpu().clamp(-1,1)+1)/2
                        aged_image = transforms.ToPILImage()(output_tensor)
                        st.image(aged_image, caption="미래 얼굴 예측", width='stretch')

                    except Exception as e:
                        st.error(f"미래 얼굴 생성 오류: {e}")
                        st.code(traceback.format_exc())
            else:
                st.error("이미지를 읽을 수 없습니다.")
        except Exception as e:
            st.error(f"이미지 처리 오류: {e}")
            st.code(traceback.format_exc())

    if st.button("홈으로 돌아가기"):
        st.session_state.page = "home"
        st.rerun()
